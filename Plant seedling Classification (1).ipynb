{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f75c753",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93278cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# this \"ImageDataGenerator\" is used for data augmentation which means increasing the no.of images by\n",
    "# rotating,zooming,flipping..etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e51f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale = (1./255),horizontal_flip=True,shear_range=0.2)\n",
    "test_gen = ImageDataGenerator(rescale = (1./255))\n",
    "\n",
    "# rescale         -> makes pixel values in [0,1]\n",
    "# horizontal_flip -> It randomly flips images horizontally, which can help the model for better training\n",
    "# shear_range     ->  # change the angle of image to make more images.Maximum angle changed is of 0.2 radians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c760560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4750 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory(\"C:\\\\Users\\\\VISWA TEJA\\\\Downloads\\\\trainplant\",\n",
    "                                      target_size = (120,120),\n",
    "                                      class_mode = 'categorical',\n",
    "                                      batch_size=8)\n",
    "\n",
    "# target_size  -> sets the size to which the images are resized.\n",
    "# class_mode   -> specifies that you are working with categorical labels.\n",
    "# batch_size   -> determines the number of images to be loaded at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5b19c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1086 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "test = test_gen.flow_from_directory(\"C:\\\\Users\\\\VISWA TEJA\\\\Downloads\\\\testplant\",\n",
    "                                     target_size = (120,120),\n",
    "                                     class_mode = 'categorical',\n",
    "                                     batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154ca126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Black-grass': 0,\n",
       " 'Charlock': 1,\n",
       " 'Cleavers': 2,\n",
       " 'Common Chickweed': 3,\n",
       " 'Common wheat': 4,\n",
       " 'Fat Hen': 5,\n",
       " 'Loose Silky-bent': 6,\n",
       " 'Maize': 7,\n",
       " 'Scentless Mayweed': 8,\n",
       " 'Shepherds Purse': 9,\n",
       " 'Small-flowered Cranesbill': 10,\n",
       " 'Sugar beet': 11}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6da75",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178d0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45044a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(20,(3,3),activation = 'relu',input_shape = (120,120,3)))\n",
    "\n",
    "# 20           -> no.of filters or kernal or feature detector\n",
    "# (3,3)        -> shape of filter\n",
    "# relu         -> activaltion function used to introduce non liniarity\n",
    "# input_shape  -> expected shape of input image matrix\n",
    "\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(45,activation='relu'))\n",
    "model.add(Dense(12,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc3d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# optimizer -> its a parameter used to update the model's weights during training\n",
    "# adam      -> Adam (Adaptive Moment Estimation) used for fast and effective weight updation for better accuracy\n",
    "# loss      -> Actual - pridected value is loss\n",
    "# categorical_crossentropy -> used for multi-class classification\n",
    "# metrices  -> moniter the process by eveluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "734f1a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "594/594 [==============================] - 106s 177ms/step - loss: 2.1937 - accuracy: 0.2552 - val_loss: 1.8602 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 106s 178ms/step - loss: 1.5417 - accuracy: 0.4526 - val_loss: 1.5139 - val_accuracy: 0.4365\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 105s 176ms/step - loss: 1.2549 - accuracy: 0.5615 - val_loss: 1.1114 - val_accuracy: 0.6169\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 105s 177ms/step - loss: 1.0782 - accuracy: 0.6352 - val_loss: 1.0198 - val_accuracy: 0.6501\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 104s 176ms/step - loss: 0.9538 - accuracy: 0.6876 - val_loss: 1.0530 - val_accuracy: 0.6759\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 104s 175ms/step - loss: 0.8417 - accuracy: 0.7232 - val_loss: 0.7953 - val_accuracy: 0.7514\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 105s 177ms/step - loss: 0.7346 - accuracy: 0.7680 - val_loss: 0.6999 - val_accuracy: 0.7781\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 105s 177ms/step - loss: 0.6566 - accuracy: 0.7947 - val_loss: 0.8745 - val_accuracy: 0.6943\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 109s 183ms/step - loss: 0.5763 - accuracy: 0.8154 - val_loss: 0.5717 - val_accuracy: 0.8048\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 125s 211ms/step - loss: 0.5143 - accuracy: 0.8385 - val_loss: 0.7729 - val_accuracy: 0.7265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b5edb5ead0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,batch_size=17,validation_data = test,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e60342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VISWA TEJA\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('plantseeds.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3202755",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcec9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7583f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 158ms/step\n",
      "9\n",
      "Shepherds Purse\n"
     ]
    }
   ],
   "source": [
    "img1 = image.load_img(\"C:\\\\Users\\\\VISWA TEJA\\\\Downloads\\\\Fat-hen_seedling.jpg\",target_size=(120,120))\n",
    "img1 = image.img_to_array(img1)\n",
    "img1 = np.expand_dims(img1,axis=0)\n",
    "pred = np.argmax(model.predict(img1))\n",
    "print(pred)\n",
    "output = ['Black-grass',\n",
    " 'Charlock',\n",
    " 'Cleavers',\n",
    " 'Common Chickweed',\n",
    " 'Common wheat',\n",
    " 'Fat Hen',\n",
    " 'Loose Silky-bent',\n",
    " 'Maize',\n",
    " 'Scentless Mayweed',\n",
    " 'Shepherds Purse',\n",
    " 'Small-flowered Cranesbill',\n",
    " 'Sugar beet']\n",
    "print(output[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e84d269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "5\n",
      "Fat Hen\n"
     ]
    }
   ],
   "source": [
    "img2 = image.load_img(\"C:\\\\Users\\\\VISWA TEJA\\\\Downloads\\\\balckgrass.jpeg\",target_size=(120,120))\n",
    "img2 = image.img_to_array(img2)\n",
    "img2 = np.expand_dims(img2,axis=0)\n",
    "pred = np.argmax(model.predict(img2))\n",
    "print(pred)\n",
    "output = ['Black-grass',\n",
    " 'Charlock',\n",
    " 'Cleavers',\n",
    " 'Common Chickweed',\n",
    " 'Common wheat',\n",
    " 'Fat Hen',\n",
    " 'Loose Silky-bent',\n",
    " 'Maize',\n",
    " 'Scentless Mayweed',\n",
    " 'Shepherds Purse',\n",
    " 'Small-flowered Cranesbill',\n",
    " 'Sugar beet']\n",
    "print(output[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e50502",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd2c17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(12,(3,3),activation='relu',input_shape=(120, 120, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Convolution2D(24,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Convolution2D(36,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(62,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(12,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92810df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 118, 118, 12)      336       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 59, 59, 12)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 57, 57, 24)        2616      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 28, 28, 24)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 36)        7812      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 13, 13, 36)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6084)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 62)                377270    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2016      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 390782 (1.49 MB)\n",
      "Trainable params: 390782 (1.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55e326a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60fc1c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "594/594 [==============================] - 110s 183ms/step - loss: 1.9116 - accuracy: 0.3425 - val_loss: 1.7754 - val_accuracy: 0.3508\n",
      "Epoch 2/50\n",
      "594/594 [==============================] - 109s 183ms/step - loss: 1.4661 - accuracy: 0.4842 - val_loss: 1.3283 - val_accuracy: 0.5009\n",
      "Epoch 3/50\n",
      "594/594 [==============================] - 110s 185ms/step - loss: 1.1857 - accuracy: 0.5832 - val_loss: 1.1593 - val_accuracy: 0.5764\n",
      "Epoch 4/50\n",
      "594/594 [==============================] - 108s 183ms/step - loss: 0.9865 - accuracy: 0.6619 - val_loss: 0.9198 - val_accuracy: 0.6786\n",
      "Epoch 5/50\n",
      "594/594 [==============================] - 109s 183ms/step - loss: 0.8548 - accuracy: 0.7067 - val_loss: 0.8448 - val_accuracy: 0.6906\n",
      "Epoch 6/50\n",
      "594/594 [==============================] - 109s 183ms/step - loss: 0.7442 - accuracy: 0.7381 - val_loss: 0.7132 - val_accuracy: 0.7468\n",
      "Epoch 7/50\n",
      "594/594 [==============================] - 121s 204ms/step - loss: 0.6515 - accuracy: 0.7726 - val_loss: 0.6853 - val_accuracy: 0.7385\n",
      "Epoch 8/50\n",
      "594/594 [==============================] - 133s 223ms/step - loss: 0.6111 - accuracy: 0.7941 - val_loss: 0.6203 - val_accuracy: 0.7726\n",
      "Epoch 9/50\n",
      "594/594 [==============================] - 113s 190ms/step - loss: 0.5458 - accuracy: 0.8120 - val_loss: 0.7854 - val_accuracy: 0.6851\n",
      "Epoch 10/50\n",
      "594/594 [==============================] - 84s 141ms/step - loss: 0.5046 - accuracy: 0.8255 - val_loss: 0.5023 - val_accuracy: 0.8039\n",
      "Epoch 11/50\n",
      "594/594 [==============================] - 76s 127ms/step - loss: 0.4502 - accuracy: 0.8482 - val_loss: 0.4757 - val_accuracy: 0.8223\n",
      "Epoch 12/50\n",
      "594/594 [==============================] - 69s 117ms/step - loss: 0.3975 - accuracy: 0.8611 - val_loss: 0.5548 - val_accuracy: 0.7993\n",
      "Epoch 13/50\n",
      "594/594 [==============================] - 70s 117ms/step - loss: 0.3771 - accuracy: 0.8674 - val_loss: 0.3993 - val_accuracy: 0.8407\n",
      "Epoch 14/50\n",
      "594/594 [==============================] - 70s 117ms/step - loss: 0.3384 - accuracy: 0.8787 - val_loss: 0.4503 - val_accuracy: 0.8379\n",
      "Epoch 15/50\n",
      "594/594 [==============================] - 69s 116ms/step - loss: 0.3402 - accuracy: 0.8848 - val_loss: 0.2996 - val_accuracy: 0.8867\n",
      "Epoch 16/50\n",
      "594/594 [==============================] - 72s 121ms/step - loss: 0.3106 - accuracy: 0.8952 - val_loss: 0.3831 - val_accuracy: 0.8619\n",
      "Epoch 17/50\n",
      "594/594 [==============================] - 72s 121ms/step - loss: 0.2938 - accuracy: 0.8989 - val_loss: 0.3014 - val_accuracy: 0.8840\n",
      "Epoch 18/50\n",
      "594/594 [==============================] - 70s 117ms/step - loss: 0.2593 - accuracy: 0.9076 - val_loss: 0.3072 - val_accuracy: 0.8821\n",
      "Epoch 19/50\n",
      "594/594 [==============================] - 71s 120ms/step - loss: 0.2497 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9208\n",
      "Epoch 20/50\n",
      "594/594 [==============================] - 71s 120ms/step - loss: 0.2363 - accuracy: 0.9162 - val_loss: 0.2900 - val_accuracy: 0.8886\n",
      "Epoch 21/50\n",
      "594/594 [==============================] - 67s 113ms/step - loss: 0.2176 - accuracy: 0.9246 - val_loss: 0.2326 - val_accuracy: 0.9107\n",
      "Epoch 22/50\n",
      "594/594 [==============================] - 67s 112ms/step - loss: 0.1965 - accuracy: 0.9324 - val_loss: 0.2006 - val_accuracy: 0.9374\n",
      "Epoch 23/50\n",
      "594/594 [==============================] - 67s 112ms/step - loss: 0.1885 - accuracy: 0.9362 - val_loss: 0.1929 - val_accuracy: 0.9401\n",
      "Epoch 24/50\n",
      "594/594 [==============================] - 68s 115ms/step - loss: 0.1896 - accuracy: 0.9358 - val_loss: 0.1649 - val_accuracy: 0.9438\n",
      "Epoch 25/50\n",
      "594/594 [==============================] - 66s 112ms/step - loss: 0.1607 - accuracy: 0.9453 - val_loss: 0.1668 - val_accuracy: 0.9392\n",
      "Epoch 26/50\n",
      "594/594 [==============================] - 69s 116ms/step - loss: 0.1662 - accuracy: 0.9415 - val_loss: 0.2197 - val_accuracy: 0.9162\n",
      "Epoch 27/50\n",
      "594/594 [==============================] - 67s 113ms/step - loss: 0.1393 - accuracy: 0.9524 - val_loss: 0.2048 - val_accuracy: 0.9236\n",
      "Epoch 28/50\n",
      "594/594 [==============================] - 67s 113ms/step - loss: 0.1495 - accuracy: 0.9505 - val_loss: 0.1963 - val_accuracy: 0.9291\n",
      "Epoch 29/50\n",
      "594/594 [==============================] - 66s 111ms/step - loss: 0.1352 - accuracy: 0.9520 - val_loss: 0.1530 - val_accuracy: 0.9521\n",
      "Epoch 30/50\n",
      "594/594 [==============================] - 66s 112ms/step - loss: 0.1269 - accuracy: 0.9566 - val_loss: 0.1060 - val_accuracy: 0.9613\n",
      "Epoch 31/50\n",
      "594/594 [==============================] - 69s 116ms/step - loss: 0.1125 - accuracy: 0.9608 - val_loss: 0.2475 - val_accuracy: 0.9180\n",
      "Epoch 32/50\n",
      "594/594 [==============================] - 71s 120ms/step - loss: 0.1257 - accuracy: 0.9562 - val_loss: 0.3388 - val_accuracy: 0.8895\n",
      "Epoch 33/50\n",
      "594/594 [==============================] - 70s 118ms/step - loss: 0.0953 - accuracy: 0.9691 - val_loss: 0.0754 - val_accuracy: 0.9770\n",
      "Epoch 34/50\n",
      "594/594 [==============================] - 69s 116ms/step - loss: 0.1271 - accuracy: 0.9562 - val_loss: 0.0874 - val_accuracy: 0.9733\n",
      "Epoch 35/50\n",
      "594/594 [==============================] - 66s 112ms/step - loss: 0.0792 - accuracy: 0.9733 - val_loss: 0.0974 - val_accuracy: 0.9650\n",
      "Epoch 36/50\n",
      "594/594 [==============================] - 66s 111ms/step - loss: 0.0972 - accuracy: 0.9651 - val_loss: 0.0984 - val_accuracy: 0.9705\n",
      "Epoch 37/50\n",
      "594/594 [==============================] - 70s 118ms/step - loss: 0.1086 - accuracy: 0.9642 - val_loss: 0.0670 - val_accuracy: 0.9779\n",
      "Epoch 38/50\n",
      "594/594 [==============================] - 73s 122ms/step - loss: 0.0827 - accuracy: 0.9737 - val_loss: 0.1393 - val_accuracy: 0.9586\n",
      "Epoch 39/50\n",
      "594/594 [==============================] - 73s 123ms/step - loss: 0.0830 - accuracy: 0.9760 - val_loss: 0.0491 - val_accuracy: 0.9843\n",
      "Epoch 40/50\n",
      "594/594 [==============================] - 75s 126ms/step - loss: 0.0901 - accuracy: 0.9737 - val_loss: 0.0586 - val_accuracy: 0.9834\n",
      "Epoch 41/50\n",
      "594/594 [==============================] - 70s 117ms/step - loss: 0.0687 - accuracy: 0.9777 - val_loss: 0.1172 - val_accuracy: 0.9595\n",
      "Epoch 42/50\n",
      "594/594 [==============================] - 70s 118ms/step - loss: 0.0604 - accuracy: 0.9806 - val_loss: 0.0658 - val_accuracy: 0.9779\n",
      "Epoch 43/50\n",
      "594/594 [==============================] - 71s 119ms/step - loss: 0.0996 - accuracy: 0.9699 - val_loss: 0.0526 - val_accuracy: 0.9871\n",
      "Epoch 44/50\n",
      "594/594 [==============================] - 70s 119ms/step - loss: 0.0718 - accuracy: 0.9760 - val_loss: 0.1160 - val_accuracy: 0.9549\n",
      "Epoch 45/50\n",
      "594/594 [==============================] - 71s 119ms/step - loss: 0.0760 - accuracy: 0.9789 - val_loss: 0.0582 - val_accuracy: 0.9834\n",
      "Epoch 46/50\n",
      "594/594 [==============================] - 70s 117ms/step - loss: 0.0318 - accuracy: 0.9907 - val_loss: 0.0160 - val_accuracy: 0.9945\n",
      "Epoch 47/50\n",
      "594/594 [==============================] - 70s 118ms/step - loss: 0.0702 - accuracy: 0.9762 - val_loss: 0.2562 - val_accuracy: 0.9217\n",
      "Epoch 48/50\n",
      "594/594 [==============================] - 70s 117ms/step - loss: 0.1010 - accuracy: 0.9657 - val_loss: 0.0963 - val_accuracy: 0.9632\n",
      "Epoch 49/50\n",
      "594/594 [==============================] - 70s 118ms/step - loss: 0.0595 - accuracy: 0.9811 - val_loss: 0.0362 - val_accuracy: 0.9853\n",
      "Epoch 50/50\n",
      "594/594 [==============================] - 72s 121ms/step - loss: 0.0629 - accuracy: 0.9789 - val_loss: 0.0240 - val_accuracy: 0.9954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b5eecad420>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,batch_size=17,validation_data=test,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8b125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 917ms/step\n",
      "0\n",
      "Black-grass\n"
     ]
    }
   ],
   "source": [
    "img3 = image.load_img(\"C:\\\\Users\\\\VISWA TEJA\\\\Downloads\\\\blackgrass.jpeg\",target_size=(120,120))\n",
    "img3 = image.img_to_array(img3)\n",
    "img3 = np.expand_dims(img3,axis=0)\n",
    "pred = np.argmax(model.predict(img3))\n",
    "print(pred)\n",
    "output = ['Black-grass',\n",
    " 'Charlock',\n",
    " 'Cleavers',\n",
    " 'Common Chickweed',\n",
    " 'Common wheat',\n",
    " 'Fat Hen',\n",
    " 'Loose Silky-bent',\n",
    " 'Maize',\n",
    " 'Scentless Mayweed',\n",
    " 'Shepherds Purse',\n",
    " 'Small-flowered Cranesbill',\n",
    " 'Sugar beet']\n",
    "print(output[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ade6d5",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8d7f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3803 images belonging to 12 classes.\n",
      "Found 947 images belonging to 12 classes.\n",
      "Epoch 1/10\n",
      "119/119 [==============================] - 275s 2s/step - loss: 2.4524 - accuracy: 0.1249 - val_loss: 2.4204 - val_accuracy: 0.1373\n",
      "Epoch 2/10\n",
      "119/119 [==============================] - 290s 2s/step - loss: 2.4290 - accuracy: 0.1265 - val_loss: 2.4219 - val_accuracy: 0.1373\n",
      "Epoch 3/10\n",
      "119/119 [==============================] - 269s 2s/step - loss: 2.4229 - accuracy: 0.1412 - val_loss: 2.4093 - val_accuracy: 0.1119\n",
      "Epoch 4/10\n",
      "119/119 [==============================] - 261s 2s/step - loss: 2.4153 - accuracy: 0.1409 - val_loss: 2.4092 - val_accuracy: 0.1309\n",
      "Epoch 5/10\n",
      "119/119 [==============================] - 261s 2s/step - loss: 2.4117 - accuracy: 0.1336 - val_loss: 2.4027 - val_accuracy: 0.1383\n",
      "Epoch 6/10\n",
      "119/119 [==============================] - 256s 2s/step - loss: 2.4059 - accuracy: 0.1430 - val_loss: 2.3996 - val_accuracy: 0.1584\n",
      "Epoch 7/10\n",
      "119/119 [==============================] - 256s 2s/step - loss: 2.4087 - accuracy: 0.1449 - val_loss: 2.3867 - val_accuracy: 0.1499\n",
      "Epoch 8/10\n",
      "119/119 [==============================] - 257s 2s/step - loss: 2.3950 - accuracy: 0.1525 - val_loss: 2.3764 - val_accuracy: 0.1573\n",
      "Epoch 9/10\n",
      "119/119 [==============================] - 260s 2s/step - loss: 2.3978 - accuracy: 0.1488 - val_loss: 2.3745 - val_accuracy: 0.1637\n",
      "Epoch 10/10\n",
      "119/119 [==============================] - 260s 2s/step - loss: 2.3952 - accuracy: 0.1412 - val_loss: 2.3831 - val_accuracy: 0.1552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VISWA TEJA\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"C:\\\\Users\\\\VISWA TEJA\\\\Downloads\\\\trainplant\",\n",
    "    target_size=(224, 224),\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=32,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    \"C:\\\\Users\\\\VISWA TEJA\\\\Downloads\\\\trainplant\",\n",
    "    target_size=(224, 224),\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=32,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# Load the ResNet-50 model pre-trained on ImageNet data\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add custom layers for plant seedling classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(12, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
    "\n",
    "# Save the model\n",
    "model.save('resnet50_plant_seedlings.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c1bb5",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab3d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90452705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb56351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d0093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a68c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef83af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
